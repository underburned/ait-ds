# Pandas. Базовые операции

1. Считать в `pandas.DataFrame` любой источник данных: CSV, JSON, Excel-файл, HTML-таблицу и т.п.
Также можно сконвертировать в DataFrame любой из встроенных датасетов `sklearn`: (см. [инструкцию](https://stackoverflow.com/questions/38105539/how-to-convert-a-scikit-learn-dataset-to-a-pandas-dataset)).  
   > Главное условие к датасету, который вы загружаете - там должны быть числовые колонки (численные признаки).

2. Выполнить над датафреймом следующие операции:  
   - `.head()`.
   - `.describe()`.
   - Считывание значения конкретной ячейки (с конкретным индексом из конкретной колонки).
   - Фильтрация строк по диапазону индекса.
   - Фильтрация набора данных по какому-либо условию.
   - Работа с пропущенными значениями:
      + удаление строк с пропущенными значениями;
      + заполнение пропущенных значений средним значением по колонке.  
     > Если пропущенных значений нет &ndash; намеренно их сгенерировать, прибить какие-то куски данных в `np.nan`.
   - Создание нового поля, вычисленного на основе значений других полей:
      + через выражение на базе имеющихся колонок;
      + через `DataFrame.apply`;
      + через `Series.apply`.
   - Сортировка по какому-либо из полей.
   - Вычисление нескольких статистик по колонкам (использовать встроенные агрегатные функции &ndash; любые на выбор).
   - Вывод по какому-либо полю / набору полей числа значений с использованием `.value_counts()`.
   - Вывод уникальных значений какой-либо колонки с использованием `.unique()`.
   - Удаление текущего индекса, и создание нового индекса на базе новой колонки, которая для этого лучше всего подходит &ndash; см. 
[reset_index](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) и 
[set_index](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html).

3. Продемонстрировать работу `.groupby`, на основе группировок в `groupby` вычислить агрегатные функции по одной или нескольким колонкам.

4. Решейпинг данных $1D\rightarrow2D$ с использованием `.pivot` (можно подать на вход результаты агрегатов, полученных ранее с использованием `.groupby` (сгруппировать по двум полям). 

5. Решейпинг $1D\rightarrow2D$ данных, соединённых с группировкой / агрегацией (одним словом &ndash; сводная таблица): `.pivot_table`. Группировать только по категориальным полям или числовым, если значений немного.
   > Если значений много, то необходимо их загрубить. Например, вычислить гистограмму как в задании (7).

6. Вычислить квантили распределения какого-либо вещественного признака (с использованием `numpy.quantile` или `numpy.percentile`).  

7. Вычислить (в виде текста) гистограмму какого-либо вещественного признака (с использованием `numpy.histogram`). Значения гистограммы можно использовать в качестве загрубленного числового признака для заданий (4) или (5).  

8. Проитерировать `DataFrame` построчно `.iterrows()` и выполнить какую-либо операцию внутри цикла.  

9. Получить `DataFrame` с `MultiIndex` любым способом: через конструктор (в документации есть множество видов конструкторов для создания `MultiIndex` с нуля), через `read_sql` / `read_csv` / `read_excel`, `read_*`, через `pivot_table`, через `groupby` или иными способами.  
   - Переставить местами уровни индекса.
   - Транспонировать таблицу (или создать новую другую) с `MultiIndex`.
   - Удалить один из уровней индекса или добавить новый уровень индекса (можно инициализированный константой) &ndash; см. документацию.

10. Продемонстировать работу `.merge`.

11. Продемонстрировать работу с `.concat` или `append`.

### Примечания

> Некоторые используют `.pivot` и `.pivot_table` неправильно или в неподходящем контексте, в результате получается бессмыслица.
> - `.pivot` нужен для того, чтобы результаты двухуровневой группировки представить в виде двумерной таблицы, где один из уровней группировки уходит в строчки, другой в столбцы.
> - `.pivot_table` нужен для того же, только умеет сам группировать / агрегировать данные, а не просто растаскивать ячейки по двумерной таблице.

> Для заданий (9) - (11) можно сгенерировать датасет из случайных данных.